{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41d4e46f",
   "metadata": {},
   "source": [
    "### Installing dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442682dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c2485f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade albumentations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e1dc23",
   "metadata": {},
   "source": [
    "### Training UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e41c924-674e-45cb-a230-40ad3b4bff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from transformers.modeling_outputs import SemanticSegmenterOutput\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "import timm\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "def get_device():\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, dataset, transform):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = np.array(item[\"image\"])\n",
    "        label = np.array(item[\"label\"])\n",
    "\n",
    "        transformed = self.transform(image=image, mask=label)\n",
    "        image, label = torch.tensor(transformed['image']).permute(2, 0, 1), torch.LongTensor(transformed['mask'])\n",
    "\n",
    "        return image, label\n",
    "\n",
    "ADE_MEAN = (np.array([123.675, 116.280, 103.530]) / 255).tolist()\n",
    "ADE_STD = (np.array([58.395, 57.120, 57.375]) / 255).tolist()\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(width=224, height=224),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=45, p=0.5),\n",
    "    A.OneOf([\n",
    "        A.GridDistortion(p=0.5),\n",
    "        A.OpticalDistortion(distort_limit=1, shift_limit=0.5, p=0.5),\n",
    "    ], p=0.3),\n",
    "    A.OneOf([\n",
    "        A.HueSaturationValue(10,15,10),\n",
    "        A.CLAHE(clip_limit=2),\n",
    "        A.RandomBrightnessContrast(),\n",
    "    ], p=0.3),\n",
    "    A.Normalize(mean=ADE_MEAN, std=ADE_STD),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(width=224, height=224),\n",
    "    A.Normalize(mean=ADE_MEAN, std=ADE_STD),\n",
    "])\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNetDecoder(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.up1 = nn.ConvTranspose2d(in_channels, 512, kernel_size=2, stride=2)\n",
    "        self.conv1 = DoubleConv(512, 256)\n",
    "        self.up2 = nn.ConvTranspose2d(256, 256, kernel_size=2, stride=2)\n",
    "        self.conv2 = DoubleConv(256, 128)\n",
    "        self.up3 = nn.ConvTranspose2d(128, 128, kernel_size=2, stride=2)\n",
    "        self.conv3 = DoubleConv(128, 64)\n",
    "        self.up4 = nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2)\n",
    "        self.conv4 = DoubleConv(64, 32)\n",
    "        self.final_conv = nn.Conv2d(32, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.up2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.up3(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.up4(x)\n",
    "        x = self.conv4(x)\n",
    "        return self.final_conv(x)\n",
    "\n",
    "class MAEForSemanticSegmentation(nn.Module):\n",
    "    def __init__(self, vit_model, num_labels):\n",
    "        super().__init__()\n",
    "        self.vit = vit_model\n",
    "        self.decoder = UNetDecoder(in_channels=self.vit.num_features, num_classes=num_labels)\n",
    "\n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        features = self.vit.forward_features(pixel_values)\n",
    "        \n",
    "        if features.shape[1] == (self.vit.patch_embed.num_patches + 1):\n",
    "            patch_embeddings = features[:, 1:, :]\n",
    "        else:\n",
    "            patch_embeddings = features\n",
    "\n",
    "        batch_size, num_patches, hidden_dim = patch_embeddings.shape\n",
    "        height = width = int((num_patches) ** 0.5)\n",
    "        feature_map = patch_embeddings.reshape(batch_size, height, width, hidden_dim).permute(0, 3, 1, 2)\n",
    "\n",
    "        logits = self.decoder(feature_map)\n",
    "\n",
    "        logits = F.interpolate(logits, size=pixel_values.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=0)\n",
    "            loss = loss_fct(logits, labels)\n",
    "\n",
    "        return SemanticSegmenterOutput(loss=loss, logits=logits)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean', ignore_index=255):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', ignore_index=self.ignore_index)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6, ignore_index=255):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred = F.softmax(y_pred, dim=1)\n",
    "        y_true_onehot = F.one_hot(y_true, num_classes=y_pred.shape[1]).permute(0, 3, 1, 2).float()\n",
    "\n",
    "        mask = (y_true != self.ignore_index).float().unsqueeze(1)\n",
    "\n",
    "        intersection = torch.sum(y_pred * y_true_onehot * mask, dim=[0, 2, 3])\n",
    "        union = torch.sum((y_pred + y_true_onehot) * mask, dim=[0, 2, 3])\n",
    "\n",
    "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader, num_epochs=100, learning_rate=5e-5, model_name=\"\", is_cam=False):\n",
    "    device = get_device()\n",
    "    print(f\"Using device: {device}\")\n",
    "    model.to(device)\n",
    "\n",
    "    for param in model.vit.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    dice_loss = DiceLoss(ignore_index=0).to(device)\n",
    "    focal_loss = FocalLoss(ignore_index=0).to(device)\n",
    "\n",
    "    best_dice = 0.0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            pixel_values, labels = batch\n",
    "            pixel_values, labels = pixel_values.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss = dice_loss(outputs.logits, labels) + focal_loss(outputs.logits, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        dice_scores = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader, desc=\"Validation\"):\n",
    "                pixel_values, labels = batch\n",
    "                pixel_values, labels = pixel_values.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(pixel_values=pixel_values)\n",
    "                loss = dice_loss(outputs.logits, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate Dice score\n",
    "                dice = 1 - dice_loss(outputs.logits, labels)\n",
    "                dice_scores.append(dice.item())\n",
    "\n",
    "        val_loss /= len(val_dataloader)\n",
    "        avg_dice = np.mean(dice_scores)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Average Dice Score: {avg_dice:.4f}\")\n",
    "\n",
    "        if avg_dice > best_dice:\n",
    "            best_dice = avg_dice\n",
    "            best_model_state = model.state_dict()\n",
    "            print(f\"New best model found with Dice Score: {best_dice:.4f}\")\n",
    "\n",
    "        if (epoch + 1) % 10 == 0 or epoch == num_epochs - 1:\n",
    "            cam_type = \"CAM\" if is_cam else \"NO_CAM\"\n",
    "            save_dir = f\"{cam_type}/{model_name}\"\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            save_path = os.path.join(save_dir, f\"checkpoint_epoch_{epoch+1}_dice_{avg_dice:.4f}.pth\")\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Checkpoint saved at epoch {epoch+1}\")\n",
    "\n",
    "    print(\"Training completed!\")\n",
    "    \n",
    "    if best_model_state is not None:\n",
    "        cam_type = \"CAM\" if is_cam else \"NO_CAM\"\n",
    "        save_dir = f\"{cam_type}/{model_name}\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        save_path = os.path.join(save_dir, f\"best_model_dice_{best_dice:.4f}.pth\")\n",
    "        torch.save(best_model_state, save_path)\n",
    "        print(f\"Best model saved with Dice Score: {best_dice:.4f}\")\n",
    "\n",
    "    return model, best_dice\n",
    "\n",
    "def main(model_list):\n",
    "    device = get_device()\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    dataset = load_dataset(\"Ayushnangia/FUGseg_dilation\")\n",
    "    train_dataset = SegmentationDataset(dataset[\"train\"], transform=train_transform)\n",
    "    val_dataset = SegmentationDataset(dataset[\"validation\"], transform=val_transform)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    id2label = {      \n",
    "        0: \"background\",\n",
    "        1: \"ulcer\",\n",
    "        2: \"rice\",\n",
    "        3: \"sausages\",\n",
    "        4: \"rice_sausages\",\n",
    "        5: \"rice_ulcer\",\n",
    "        6: \"sausages_ulcer\",\n",
    "    }\n",
    "\n",
    "    for model_info in model_list:\n",
    "        weights_path = model_info['weights_path']\n",
    "        is_cam = model_info['is_cam']\n",
    "        \n",
    "        model_name = os.path.basename(weights_path)\n",
    "        \n",
    "        print(f\"Training model: {model_name} ({'CAM' if is_cam else 'NO_CAM'})\")\n",
    "\n",
    "        vit_model = timm.create_model('vit_base_patch16_224', pretrained=False)\n",
    "\n",
    "        state_dict = torch.load(weights_path, map_location=device)\n",
    "        vit_model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "        model = MAEForSemanticSegmentation(vit_model, num_labels=len(id2label))\n",
    "\n",
    "        _, best_dice = train_model(model, train_dataloader, val_dataloader, num_epochs=100, learning_rate=5e-5, model_name=model_name, is_cam=is_cam)\n",
    "\n",
    "        print(f\"Best Dice Score for {model_name}: {best_dice:.4f}\")\n",
    "        print(\"-----------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be302f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\n",
    "    {\n",
    "        'weights_path': '// path for weight path CAM MAE',\n",
    "        'is_cam': True\n",
    "    },\n",
    "    {\n",
    "        'weights_path': '// path for weight path NON CAM MAE',\n",
    "        'is_cam': False\n",
    "    }\n",
    "]\n",
    "main(model_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d09d60",
   "metadata": {},
   "source": [
    "### Inference code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a55cf822-e61e-484b-bec8-9dd26667b320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_model = timm.create_model('vit_base_patch16_224', pretrained=False)\n",
    "\n",
    "state_dict = torch.load(\"// path to MAE finetuned model\")\n",
    "\n",
    "id2label = {\n",
    "    0: \"background\",\n",
    "    1: \"ulcer\",\n",
    "    2: \"rice\",\n",
    "    3: \"sausages\",\n",
    "    4: \"rice_sausages\",\n",
    "    5: \"rice_ulcer\",\n",
    "    6: \"sausages_ulcer\",\n",
    "}\n",
    "model = MAEForSemanticSegmentation(vit_model, num_labels=len(id2label))\n",
    "\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "25cb3fc7-0897-42d0-ac63-f81b33af99c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, image_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    image = Image.open(image_path)\n",
    "    image_np = np.array(image)\n",
    "\n",
    "    transformed = val_transform(image=image_np)\n",
    "    image_tensor = torch.tensor(transformed['image']).permute(2, 0, 1).unsqueeze(0).float().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=image_tensor)\n",
    "\n",
    "    upsampled_logits = F.interpolate(outputs.logits, size=image.size[::-1], mode=\"bilinear\", align_corners=False)\n",
    "    predicted_map = upsampled_logits.argmax(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "    return image, predicted_map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "580bf2fc-62a5-40a7-a3cc-d1dc30b2ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "def visualize_ulcer_overlay(image, predicted_map, id2label):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    if isinstance(image, Image.Image):\n",
    "        image = np.array(image)\n",
    "\n",
    "    if len(image.shape) == 2:  \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    elif image.shape[2] == 4:  \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
    "\n",
    "    ulcer_mask = (predicted_map == 1).astype(np.uint8)\n",
    "\n",
    "    overlay = np.zeros((*image.shape[:2], 3), dtype=np.uint8)\n",
    "    overlay[ulcer_mask == 1] = [0, 255, 0]  \n",
    "\n",
    "    alpha = 0.5 \n",
    "    blended = cv2.addWeighted(image, 1, overlay, alpha, 0)\n",
    "\n",
    "    plt.imshow(blended)\n",
    "    plt.title(\"Original Image with Ulcer Overlay\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor='green', edgecolor='green', label='Ulcer')]\n",
    "    plt.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9474f1f-d191-494c-bc9b-389891759003",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"// path to ulcer image\"\n",
    "image, predicted_map = inference(model, image_path)\n",
    "\n",
    "visualize_ulcer_overlay(image, predicted_map, id2label)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
